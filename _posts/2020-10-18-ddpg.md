---
layout: post
title: DDPG understanding
---
## 
DDPG已经普遍应用在机器人领域。
首先这句话摘自OPENAI 的DDPG Doc中。很简单清晰度哦解释了DDPG。
Deep Deterministic Policy Gradient (DDPG) is an algorithm which concurrently learns a Q-function and a policy. It uses off-policy data and the Bellman equation to learn the Q-function, and uses the Q-function to learn the policy.

现在我将总结几点，我在理解时觉得困难的点.
1. loss function
1.1 Critic network / value network / Q network
 Value网络基于Q learning，希望可以学到输入状态s和动作a可以生成对应的Q值。
 
$$e=mc^2$$

2.  注意这里有两个value metwork，一个是original value network， 还有一个target value network。
target network并不是步步更新的，而original value network是步步更新。


## Resources
- [original paper](https://arxiv.org/pdf/1509.02971.pdf)
- [openAI DDPG Doc](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)
- [DDPG (tensorflow)](https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html)
- [DDPG jupter tutorial (pytorch)](https://github.com/antocapp/paperspace-ddpg-tutorial)
- [Coding analysis of DDPG Actor ](https://be-my-only.xyz/blog/DDPG/) (in Chinese)
