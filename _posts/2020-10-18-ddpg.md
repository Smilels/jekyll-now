---
layout: post
title: DDPG understanding
---
 
DDPG已经普遍应用在机器人领域。
下面这句话摘自OPENAI 的DDPG Doc中，它非常清晰明了得解释了DDPG。
Deep Deterministic Policy Gradient (DDPG) is an algorithm which concurrently learns a $$Q$$-function and a policy. It uses off-policy data and the Bellman equation to learn the $$Q$$-function, and uses the $$Q$$-function to learn the policy.

现在我将总结几点，自己在理解时觉得困难的几点。
1. 在DDPG中存在两个policy network和value network。这两个network一个是original network， 还有一个target network。
Target网络的更新是通过？？实现的。
在原论文中的，这种设计的解释是提高了Q网络的稳定性。


2. DDPG为何可以实现连续动作的学习？
如果是该任务有有限个离散的动作，那么我们可以直接计算每个动作的Q值。但是如果动作是连续的，我们便无法详尽地评估每个动作的Q值，因此此类优化问题便不再简单。
但正因动作空间是连续的，所以$$Q^*(s,a)$$ 可以假定为在动作空间可导。因此我们可以使用一个基于梯度的policy网络$$\mua(s)$$来近似动作$$a$$.

3. loss function of Critic network / value network / $$Q$$ network
 Value网络基于$$Q$$ learning，希望可以学到输入状态s和动作a可以生成对应的Q值。
$$e=mc^2$$
其中$$y_i$$是使用两个target网络更新的，而目标的 $$Q$$ 值是使用orginal value network求得的。
这样loss的目标是希望当前value network求得的 $$Q$$ value能与？？？？越来越相近。

4. loss function of policy/actor network
其实actor网络的loss很好理解，就是希望Q值越大越好，希望根据梯度上升的方法。
其中第二项 $$\mu$$ 梯度就是根据链式法则的来的，因为 $$a=\mu(s)$$ ,因为对 $$\theta_mu$$ 求导，可以展开为先对 $$a$$ 求导，再对 $$\theta_mu$$ 求导。

5. 根据博客[Coding analysis of DDPG Actor ](https://be-my-only.xyz/blog/DDPG/)的分析，让我明白了pytorch中如何写actor loss。
要非常注意在定义loss时更新的是actor的参数。




## Resources
- [original paper](https://arxiv.org/pdf/1509.02971.pdf)
- [openAI DDPG Doc](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)
- [DDPG (tensorflow)](https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html)
- [DDPG jupter tutorial (pytorch)](https://github.com/antocapp/paperspace-ddpg-tutorial)
- [Coding analysis of DDPG Actor ](https://be-my-only.xyz/blog/DDPG/) (in Chinese)
